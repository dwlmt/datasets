# WritingPrompts   

## Source

The WritingPrompts dataset from:

@inproceedings{fan-etal-2018-hierarchical,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
}

**The abstract:** "We explore story generation: 
creative systems that can build coherent and fluent passages of text 
about a topic. We collect a large dataset of 300K human-written stories 
paired with writing prompts from an online forum. Our dataset enables 
hierarchical story generation, where the model first generates a premise, 
and then transforms it into a passage of text. We gain further 
improvements with a novel form of model fusion that improves the 
relevance of the story to the prompt, and adding a new gated 
multi-scale self-attention mechanism to model long-range context. 
Experiments show large improvements over strong baselines on both 
automated and human evaluations. Human judges prefer stories 
generated by our approach to those from a strong non-hierarchical 
model by a factor of two to one."

## Dataset

Just the collates the WritingPrompts dataset into 3 different compressed jsonl lines.
The main reason is the prompts are and text are together to make it simpler to
work with downstream. The format of each json line:

- **id:** The line number from the original final, zero indexed.
- **title:** The prompt from the source file. Title is to be more consistent with other NLP datasets.
- **text:** The full text of the story.

Additionally the _\<newline>_  from the original dataset has been replaced by _\n_. 

